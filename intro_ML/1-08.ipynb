{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 10: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dataset(s) needed: MNIST (\"Modified National Institute of Standards and Technology\") dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the MNIST dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Q.1. Split the data into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing).\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #TODO\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     mnist.data, \n",
    "#     mnist.target, \n",
    "#     test_size=10000, \n",
    "#     random_state=99\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.data[:60000]\n",
    "X_test = mnist.data[60000:]\n",
    "y_train = mnist.target[:60000]\n",
    "y_test = mnist.target[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "for subset in [X_train, X_test, y_train, y_test]:\n",
    "    print(subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.2. Train a Logistic Regression classifier on the dataset and see how long it takes.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 235.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=None)]: Done  10 out of  10 | elapsed:  3.9min finished\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver='lbfgs', verbose=1, max_iter=500, multi_class='ovr')\n",
    "start_time = time.time()\n",
    "#TODO: Train the classifier\n",
    "log_clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Training took {:.2f}s\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.3. Evaluate the resulting model on the test set.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97       980\n",
      "           1       0.97      0.98      0.97      1135\n",
      "           2       0.93      0.88      0.91      1032\n",
      "           3       0.90      0.91      0.91      1010\n",
      "           4       0.93      0.92      0.93       982\n",
      "           5       0.91      0.85      0.88       892\n",
      "           6       0.94      0.95      0.94       958\n",
      "           7       0.92      0.92      0.92      1028\n",
      "           8       0.84      0.88      0.86       974\n",
      "           9       0.90      0.89      0.90      1009\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, log_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Q.4. Use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9502650943304869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n = 155\n",
    "pca = PCA(n_components=n)\n",
    "pca.fit(X_train)\n",
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Q.5. Train a new Logistic Regression classifier on the reduced dataset and see how long it takes. Was training much faster? Explain your results.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 27.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=None)]: Done   1 out of   1 | elapsed:   27.2s finished\n"
     ]
    }
   ],
   "source": [
    "X_train_pca = pca.transform(X_train)\n",
    "\n",
    "log_clf = LogisticRegression(\n",
    "    solver='lbfgs', \n",
    "    verbose=1, \n",
    "    max_iter=300, \n",
    "    multi_class='auto'\n",
    ")\n",
    "start_time = time.time()\n",
    "#TODO: Train the classifier\n",
    "log_clf.fit(X_train_pca, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Training took {:.2f}s\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.6. Evaluate the new classifier on the test set: how does it compare to the previous classifier? Discuss the speed / accuracy trade-off and in which case you'd prefer a very slight drop in model performance for a x-time speedup in training.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       980\n",
      "           1       0.95      0.98      0.97      1135\n",
      "           2       0.93      0.91      0.92      1032\n",
      "           3       0.91      0.90      0.91      1010\n",
      "           4       0.92      0.93      0.92       982\n",
      "           5       0.91      0.86      0.89       892\n",
      "           6       0.94      0.95      0.94       958\n",
      "           7       0.93      0.93      0.93      1028\n",
      "           8       0.88      0.87      0.88       974\n",
      "           9       0.89      0.91      0.90      1009\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, log_clf.predict(pca.transform(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance gains of dimensionality reduction were here very considerable: ~30 seconds vs ~203 seconds, or something like 7x faster. Quite the improvement!\n",
    "\n",
    "Meanwhile, the tradeoff in performance was pretty small. Some classes so no decrease in classification performance (such as digit 0). Others saw minimal decreases; digit 9, for example, saw a decrease in F1 score of only .01. \n",
    "\n",
    "In this case, the tradeoff almost definitely seems worth it for any practical application, unless we knew for some reason that we would only be fitting this model a single time rather than update it as we collect new data. In that case, we would probably tolerate the slower fitting time in return for the small improvement of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.7. Create a new text cell in your Notebook: Complete a 50-100 word summary \n",
    "    (or short description of your thinking in applying this week's learning to the solution) \n",
    "     of your experience in this assignment. Include:\n",
    "<br>                                                                    \n",
    "What was your incoming experience with this model, if any?\n",
    "what steps you took, what obstacles you encountered.\n",
    "how you link this exercise to real-world, machine learning problem-solving. (What steps were missing? What else do you need to learn?)\n",
    "This summary allows your instructor to know how you are doing and allot points for your effort in thinking and planning, and making connections to real-world work.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I have some experience using PCA, primarily with text data. It's the most popular way of visualizing high-dimensional text data in two dimensions. What was interesting in this assignment was the question of how to systematically select your number of components n. In visualization, the answer is easy: it's usually 2, or sometimes 3. Here, however, I had to do a guided search over all possible n such that 1 => n > 784 and select the first n where the explained variance ratio is over 95%. \n",
    "\n",
    "I wonder if there is a clever way to perform that search systematically. The first option that occurs to me is to do a binary search. This is possible because, technically, our results are sorted: any increase in n will always increase our explained variance ratio. So start with n = 784/2 = 392, see if we are too high or too low, then split the search space in half again, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
